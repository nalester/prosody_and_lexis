---
title: "Analysis"
author: "Nicholas A. Lester"
date: "8/17/2021"
output:
  github_document:
    pandoc_args: --webtex
  pandoc_args: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, cache = T, cache.lazy = F, warning = F, message = F, error = F)
```


# Prosodic structure and the lexicon in interaction: a corpus-based approach
  
The following code was used to generate the models reported in the manuscript. There are three model types:

- model evaluating length as a function of prosodic position  
  
- models testing the relative pair-wise preference of words to specific prosodic position  
  
- model predicting the duration of a word in each of the three prosodic positions as a function of the pertinent variables
  
## 0. Preliminaries
**0.1 Clear memory (un-comment to use)**
```{r clear_mem}
# rm(list=ls(all=T))
```
  
**0.2 Install/load necessary libraries**
```{r load_libs}
list.of.packages =  c("lmerTest", 
                      "ggplot2", 
                      "visreg", 
                      "mgcv", 
                      "voxel", 
                      "effects", 
                      "languageR", 
                      "fastICA",
                      "dplyr")
new.packages = list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)){
    install.packages(new.packages)
}

invisible(lapply(list.of.packages, library, character.only = TRUE))
```
  
**0.3 Load data**
```{r load_data}
mod_dat = read.table("../Data/data_for_pub.txt", header=T, sep="\t", comment.char="", quote="")
```

## 1. Descriptive stats
This section produces graphs of the type/token counts per position in our sample. 
  
**1.1 Type counts per position**
```{r type_ct_per_position}
a = length(unique(mod_dat$Noun[mod_dat$facPos=="initial"]))
b = length(unique(mod_dat$Noun[mod_dat$facPos=="final"]))
c = length(unique(mod_dat$Noun[mod_dat$facPos=="medial"]))
# Note that the numbers differ because the AllThreePos condition was
# decided based on lemmas

types = data.frame(value = c(a, c, b), type = c("initial", "medial", "final"), o = c(1, 2, 3))

type.ct.plot = ggplot(data = types, aes(x=reorder(type, o), y=value, label=value)) +
  geom_bar(stat="identity", fill = "steelblue", color="black") +
  ylab("Count") +
  xlab("Position") + 
  ggtitle("Type Counts (Lemmas)") +
  theme_bw() +
  geom_text(vjust=-1) +
  ylim(0, 250) +
  theme(plot.title = element_text(hjust = 0.5), text=element_text(size=20, family = "Times New Roman"))

type.ct.plot

ggsave(type.ct.plot, file="../Results/type_count_plot.png", units = "in", width = 5, height = 3.5)
```
  
**1.2 Token counts per position**
```{r token_ct_per_position}
d = length(mod_dat$Noun[mod_dat$facPos=="initial"])
e = length(mod_dat$Noun[mod_dat$facPos=="final"])
f = length(mod_dat$Noun[mod_dat$facPos=="medial"])

tokens = data.frame(value = c(d, f, e), type = c("initial", "medial", "final"), o = c(1, 2, 3))

token.ct.plot = ggplot(data = tokens, aes(x=reorder(type, o), y=value, label=value)) +
  geom_bar(stat="identity", fill = "steelblue", color = "black") +
  ylab("Count") +
  xlab("Position") + 
  ggtitle("Token Counts") +
  theme_bw() +
  geom_text(vjust=-1) +
  ylim(0, 2700) +
  theme(plot.title = element_text(hjust = 0.5), text=element_text(size=20, family = "Times New Roman"))

token.ct.plot

ggsave(token.ct.plot, file="../Results/token_count_plot.png", units = "in", width = 5, height = 3.5)
```
  

## 2. Modeling difference in length between prosodic positions
Here we predict the duration of nouns in each of the prosodic positions (initial, medial, or final; with the caveat that the nouns are produced in DET + N constructions).  
  
Outliers were defined as nouns that took longer than 750 ms to produce. 
```{r modeling_length_between_positions}
# Descriptive
tapply(mod_dat$target.duration.ms, mod_dat$facPos, function(x) return(list(mean(x), sd(x))))

# How many outliers?
out.ct = mod_dat %>% filter(target.duration.ms>=750) %>%
         nrow()

out.prop = (out.ct/nrow(mod_dat)*100)

# Relevel predictors
len.dat = mod_dat[mod_dat$target.duration.ms<750,]
len.dat$facPos = factor(len.dat$facPos, levels = c("initial", "medial", "final"))

length.by.position = bam(target.duration.ms ~ facPos + NPhon_c + s(unsDENS_c) + s(unsLCPOSPAV_c) + s(IU_len_ms_c) + s(IU.speech.rate.graph.per.sec_c) + s(unsBPAV_c) + s(Speaker, bs="re") + s(Lemma, bs="re"), data = len.dat) 
```
  
**2.1 Results**
```{r length_between_positions_results}
anova(length.by.position)
len.mod.sum = summary(length.by.position); len.mod.sum

write.table(len.mod.sum$p.table, file="../Results/len.btw.pos_results.txt", sep="\t", quote=F)

write.table(len.mod.sum$s.table, file="../Results/len.btw.pos_results.txt", sep="\t", quote=F, append=T)
```
  
**2.2 Plotting**
```{r length_between_positions_plot}
coeff = summary(length.by.position)$p.coeff[1:3]
coeff[2] = coeff[1]+coeff[2]
coeff[3] = coeff[1]+coeff[3]

ses = summary(length.by.position)$se[1:3]

int_eff = data.frame(fit = coeff, se = ses, type = c("initial", "medial", "final"))

int_eff$type = factor(int_eff$type,levels(int_eff$type)[c(1, 3, 2)])

len.by.pos.plot = ggplot(int_eff, aes(type, fit)) +
  geom_point(color = "darkblue", size = 3) + 
  geom_errorbar(aes(ymin=fit-se, ymax=fit+se), 
                width=.4, size = 2, color="darkblue") + 
  theme_bw(base_size=24) +
  labs(color = "Position in IU", title= element_blank()) +
  ylab("Word duration (ms)") +
  theme(plot.title = element_text(hjust = 0.5), legend.position="top", legend.title = element_blank(), axis.title.y=element_blank(), legend.spacing.x = unit(0.5, 'cm'), text = element_text(family="Times New Roman")) +
  coord_flip()

len.by.pos.plot

ggsave(len.by.pos.plot, file="../Results/len_by_pos_plot.png", units = "in", width = 4, height = 3.5)
```
  
## 3. Modeling difference in positioinal preferences of words
Here we test whether the syntactic information carried by words influences where they appear in prosodic phrases.
  
**3.1 Medial vs. final**
```{r medial_vs_final_glm}
medial.vs.final = bam(facPos ~ Det + Animacy.simplified + s(lexICA1) + s(lexICA2) + s(lexICA3) + s(lexICA4) + s(lexICA5) + s(lexICA6) + s(lexICA7) + s(lexICA8) + s(lexICA9) + s(BigSurp_c) + s(Speaker,  bs="re") + s(Lemma, bs="re"), data = mod_dat[mod_dat$facPos!="initial",], family="binomial")
```
  
*3.1.1 Results*
```{r medial_vs_final_glm_results}
m.v.f.results = summary(medial.vs.final); m.v.f.results
anova(medial.vs.final)

write.table(m.v.f.results$p.table, file="../Results/m.v.f_results.txt", sep="\t", quote=F)

write.table(m.v.f.results$s.table, file="../Results/m.v.f_results.txt", sep="\t", quote=F, append=T)
```
  
*3.1.2 Plotting*
```{r medial_vs_final_glm_plot}
# Construct hypothetical dataset (non-target variables held at median or alphabetically prior level)
orig_data = mod_dat[mod_dat$facPos!="initial",]
new_data = with(orig_data, 
                expand.grid(lexICA4 = seq(min(lexICA4), 
                                              max(lexICA4), 
                                              length = 10),
                            lexICA1 = median(lexICA1),
                            lexICA2 = median(lexICA2),
                            lexICA3 = median(lexICA3),
                            lexICA5 = median(lexICA5),
                            lexICA6 = median(lexICA6),
                            lexICA7 = median(lexICA7),
                            lexICA8 = median(lexICA8),
                            lexICA9 = median(lexICA9),
                            BigSurp_c = median(BigSurp_c),
                            Det = "a",
                            Speaker = "AL",
                            Lemma = "air",
                            Animacy.simplified = "inanimate"))

# Define the link function for back-transforming
ilink = family(medial.vs.final)$linkinv

# Generate predicted values in log-odds (without random effects)
pred = predict(medial.vs.final, 
               new_data, 
               type = "link", 
               se.fit = TRUE, 
               exclude = c("s(Speaker)", "s(Lemma)", "Animacy.simplified", "Det"))

# Add the predicted values to our hypothetical dataframe
pred = cbind(pred, new_data)

# Back-transform fitted values and CIs into the response scale (i.e., probabilities rather than log-odds)
pred = transform(pred, lwr_ci = ilink(fit - (2 * se.fit)),
                       upr_ci = ilink(fit + (2 * se.fit)),
                       fitted = ilink(fit))

# Plot the result
m.v.f = ggplot(pred, aes(x = lexICA4, y = fitted)) +
        geom_ribbon(aes(ymin = lwr_ci, ymax = upr_ci), 
                    fill = "dodgerblue",
                    color = "dodgerblue",
                    alpha = 0.2) +
        geom_line() +
        geom_hline(yintercept=.5, lty=2, color="grey50") +
        ylim(0,1) +
        annotate("text", 
                 x = -2, 
                 y = .85, 
                 label = "medial", 
                 family = "Times New Roman", 
                 hjust=0) +
        annotate("text", 
                 x=-2, 
                 y=.15, 
                 label = "final", 
                 family="Times New Roman", 
                 hjust=0) +
        xlab("Component 4 (syntactic diversity →)") +
        ylab("Probability") +
        theme_bw() +
         theme(text = element_text(family="Times New Roman",
                                   size=14,
                                   face="bold"))

m.v.f

ggsave(m.v.f, file = "../Results/medial_vs_final_plot.png", units="in", width = 5, height = 3.5, device = "png")
```

**3.2 Initial vs. medial**
```{r initial_vs_medial_glm}
initial.vs.medial = bam(facPos ~ Det + Animacy.simplified + s(lexICA1) + s(lexICA2) + s(lexICA3) + s(lexICA4) + s(lexICA5) + s(lexICA6) + s(lexICA7) + s(lexICA8) + s(lexICA9) + s(BigSurp_c) + s(Speaker,  bs="re") + s(Lemma, bs="re"), data=mod_dat[mod_dat$facPos!="final",], family="binomial")
```
  
*3.2.1 Results*
```{r initial_vs_medial_glm_results}
i.v.m.results = summary(initial.vs.medial); i.v.m.results
anova(initial.vs.medial)

write.table(i.v.m.results$p.table, file="../Results/i.v.m_results.txt", sep="\t", quote=F)

write.table(i.v.m.results$s.table, file="../Results/i.v.m_results.txt", sep="\t", quote=F, append=T)
```
  
*3.2.2 Plotting*
```{r initial_vs_medial_glm_plot}
# Construct hypothetical dataset (non-target variables held at median or alphabetically prior level)
orig_data = mod_dat[mod_dat$facPos!="final",]
new_data = with(orig_data, 
                expand.grid(lexICA4 = seq(min(lexICA4), 
                                              max(lexICA4), 
                                              length = 10),
                            lexICA1 = median(lexICA1),
                            lexICA2 = median(lexICA2),
                            lexICA3 = median(lexICA3),
                            lexICA5 = median(lexICA5),
                            lexICA6 = median(lexICA6),
                            lexICA7 = median(lexICA7),
                            lexICA8 = median(lexICA8),
                            lexICA9 = median(lexICA9),
                            BigSurp_c = median(BigSurp_c),
                            Det = "a",
                            Speaker = "AL",
                            Lemma = "air",
                            Animacy.simplified = "inanimate"))

# Define the link function for back-transforming
ilink = family(initial.vs.medial)$linkinv

# Generate predicted values in log-odds (without random effects)
pred = predict(initial.vs.medial, 
               new_data, 
               type = "link", 
               se.fit = TRUE, 
               exclude = c("s(Speaker)", "s(Lemma)", "Det", "Animacy.simplfied"))

# Add the predicted values to our hypothetical dataframe
pred = cbind(pred, new_data)

# Back-transform fitted values and CIs into the response scale (i.e., probabilities rather than log-odds)
pred = transform(pred, lwr_ci = ilink(fit - (2 * se.fit)),
                       upr_ci = ilink(fit + (2 * se.fit)),
                       fitted = ilink(fit))

# Plot the result
i.v.m = ggplot(pred, aes(x = lexICA4, y = fitted)) +
        geom_ribbon(aes(ymin = lwr_ci, ymax = upr_ci), 
                    fill = "dodgerblue",
                    color = "dodgerblue",
                    alpha = 0.2) +
        geom_line() +
        geom_hline(yintercept=.5, lty=2, color="grey50") +
        ylim(0,1) +
        annotate("text", 
                 x = -2, 
                 y = .75, 
                 label = "medial", 
                 family = "Times New Roman", 
                 hjust=0) +
        annotate("text", 
                 x=-2, 
                 y=.25, 
                 label = "initial", 
                 family="Times New Roman", 
                 hjust=0) +
        xlab("Component 4 (syntactic diversity →)") +
        ylab("Probability") +
        theme_bw() +
         theme(text = element_text(family="Times New Roman",
                                   size=14,
                                   face="bold"))

i.v.m

ggsave(i.v.m, file = "../Results/initial_vs_medial_plot.png", units="in", width = 5, height = 3.5, device = "png")

```

**3.3 Initial vs. final**
```{r initial_vs_final_glm}
initial.vs.final = bam(facPos ~ Det + Animacy.simplified + s(lexICA1) + s(lexICA2) + s(lexICA3) + s(lexICA4) + s(lexICA5) + s(lexICA6) + s(lexICA7) + s(lexICA8) + s(lexICA9) + s(BigSurp_c) + s(Speaker,  bs="re") + s(Lemma, bs="re"), data=mod_dat[mod_dat$facPos!="medial",], family="binomial")
```
  
*3.3.1 Results*
```{r initial_vs_final_glm_results}
i.v.f.results = summary(initial.vs.final); i.v.f.results
anova(initial.vs.final)

write.table(i.v.f.results$p.table, file="../Results/i.v.f_results.txt", sep="\t", quote=F)

write.table(i.v.f.results$s.table, file="../Results/i.v.f_results.txt", sep="\t", quote=F, append=T)
```
  
*3.3.2 Plotting*
```{r initial_vs_final_glm_plot}
# Construct hypothetical dataset (non-target variables held at median or alphabetically prior level)
orig_data = mod_dat[mod_dat$facPos!="medial",]
new_data = with(orig_data, 
                expand.grid(lexICA4 = seq(min(lexICA4), 
                                              max(lexICA4), 
                                              length = 10),
                            lexICA1 = median(lexICA1),
                            lexICA2 = median(lexICA2),
                            lexICA3 = median(lexICA3),
                            lexICA5 = median(lexICA5),
                            lexICA6 = median(lexICA6),
                            lexICA7 = median(lexICA7),
                            lexICA8 = median(lexICA8),
                            lexICA9 = median(lexICA9),
                            BigSurp_c = median(BigSurp_c),
                            Det = "a",
                            Speaker = "AL",
                            Lemma = "air",
                            Animacy.simplified = "inanimate"))

# Define the link function for back-transforming
ilink = family(initial.vs.final)$linkinv

# Generate predicted values in log-odds (without random effects)
pred = predict(initial.vs.final, 
               new_data, 
               type = "link", 
               se.fit = TRUE, 
               exclude = c("s(Speaker)", "s(Lemma)", "Det", "Animacy.simplified"))

# Add the predicted values to our hypothetical dataframe
pred = cbind(pred, new_data)

# Back-transform fitted values and CIs into the response scale (i.e., probabilities rather than log-odds)
pred = transform(pred, lwr_ci = ilink(fit - (2 * se.fit)),
                       upr_ci = ilink(fit + (2 * se.fit)),
                       fitted = ilink(fit))

# Plot the result
i.v.f = ggplot(pred, aes(x = lexICA4, y = fitted)) +
        geom_ribbon(aes(ymin = lwr_ci, ymax = upr_ci), 
                    fill = "dodgerblue",
                    color = "dodgerblue",
                    alpha = 0.2) +
        geom_line() +
        geom_hline(yintercept=.5, lty=2, color="grey50") +
        ylim(0,1) +
        annotate("text", 
                 x = -2, 
                 y = .75, 
                 label = "initial", 
                 family = "Times New Roman", 
                 hjust=0) +
        annotate("text", 
                 x=-2, 
                 y=.25, 
                 label = "final", 
                 family="Times New Roman", 
                 hjust=0) +
        xlab("Component 4 (syntactic diversity →)") +
        ylab("Probability") +
        theme_bw() +
         theme(text = element_text(family="Times New Roman",
                                   size=14,
                                   face="bold"))

i.v.f

ggsave(i.v.f, file = "../Results/initial_vs_final_plot.png", units="in", width = 5, height = 3.5, device = "png")

```

**3.4 Correct p-values for multiple comparisons (Benjamini-Yekutieli method)**
Because we are performing multiple tests on the same data, we correct the p-values from all models.
  
*3.4.1 Get p-values from each model*
```{r get_ps}
## Medial vs. final
test1 = summary(medial.vs.final)

## Initial vs. medial
test2 = summary(initial.vs.medial)

## Initial vs. final
test3 = summary(initial.vs.final)
```
  
*3.4.2 Concatenate all p's*
```{r concatenate_ps}
pvals = c(test1$p.pv, test1$s.pv[1:10], test2$p.pv, test2$s.pv[1:10], test3$p.pv, test3$s.pv[1:10])
```
  
*3.4.3 Adjust p-values with B-Y (false-discovery rate) correction and add to data*
```{r adjust_ps}
adjustedPs = p.adjust(pvals, method="BY")

## 3.4.6 Put adjusted p's into a human-readable table
model.label.1 = rep("medial_vs_final", 14)
model.label.2 = rep("initial_vs_medial", 14)
model.label.3 = rep("initial_vs_final", 14)

model.labels = c(model.label.1, model.label.2, model.label.3)

p.df = data.frame(Model = model.labels, Variable = rep(c("Intercept", "Determiner: an", "Determiner: the", "Animacy: inanimate", "Component 1", "Component 2", "Component 3", "Component 4", "Component 5", "Component 6", "Component 7", "Component 8", "Component 9", "Bigram surprisal"), 3), p.values = round(pvals, 3), corrected.p.values = round(adjustedPs, 3))

p.df

write.table(p.df, file = "../Results/glmer_adjusted_pvalues.txt", sep = "\t", row.names = F, quote = F)

# Adjusting only p-values for the critical predictors
crit.pvals = c(test1$s.pv[c(2,4:5)], test2$s.pv[c(2,4:5)], test3$s.pv[c(2,4:5)])

crit.adjustedPs = p.adjust(crit.pvals, method="BY")
crit.mod.labels = c(rep(c("medial_vs_final", "initial_vs_medial", "initial_vs_final"), each=3)) 

crit.p.df = data.frame(Model = crit.mod.labels, Variable = rep(c("Component 2", "Component 4", "Component 5"), each = 3), p.values = round(crit.pvals, 3), corrected.p.values = round(crit.adjustedPs, 3))

crit.p.df

write.table(crit.p.df, file = "../Results/glmer_adjusted_pvalues_critical_vars_only.txt", sep = "\t", row.names = F, quote = F)
```
  
## 4. Modeling differences in duration within positions
Finally, we look inside each prosodic position to see if the lexico-syntactic variables play a role in determining duration.
  
Overly long nouns (> 750 ms) were removed prior to the analysis.
```{r modeling_duration_in_position}
duration.in.position = bam(target.duration.ms ~ Det + Animacy.simplified + s(lexICA1, by=facPos) + s(lexICA2, by=facPos) + s(lexICA3, by=facPos) + s(lexICA4, by=facPos) + s(lexICA5, by=facPos) + s(lexICA6, by=facPos) + s(lexICA7, by=facPos) + s(lexICA8, by=facPos) + s(lexICA9, by=facPos) + s(IU.speech.rate.graph.per.sec_c, by=facPos) + s(IU_len_ms_c, by=facPos) + s(BigSurp_c, by=facPos) + s(Lemma, bs="re") + s(Speaker, bs="re"), data=mod_dat[mod_dat$target.duration.ms<750,])

# With syllable-based IU speech rate measure
duration.in.position.wSyll = bam(target.duration.ms ~ Det + Animacy.simplified + s(lexICA1, by=facPos) + s(lexICA2, by=facPos) + s(lexICA3, by=facPos) + s(lexICA4, by=facPos) + s(lexICA5, by=facPos) + s(lexICA6, by=facPos) + s(lexICA7, by=facPos) + s(lexICA8, by=facPos) + s(lexICA9, by=facPos) + s(IU.speech.rate.syll.per.sec_c, by=facPos) + s(IU_len_ms_c, by=facPos) + s(BigSurp_c, by=facPos) + s(Lemma, bs="re") + s(Speaker, bs="re"), data=mod_dat[mod_dat$target.duration.ms<750,])
```
  
**4.1 Results**
```{r duration_in_position_results}
duration.results = summary(duration.in.position); duration.results
anova(duration.in.position)

write.table(duration.results$p.table, file="../Results/duration_in_position_results.txt", sep="\t", quote=F)

write.table(duration.results$s.table, file="../Results/duration_in_position_results.txt", sep="\t", quote=F, append=T)

# With syllable-based IU speech rate measure
anova(duration.in.position.wSyll)
```
  
**4.2 Plotting**  
*4.2.1 Component 5, initial position*
```{r duration_in_pos_initial_plot}
# Construct hypothetical dataset (non-target variables held at median or alphabetically prior level)
orig_data = mod_dat
new_data = with(orig_data, 
                expand.grid(lexICA5 = seq(min(lexICA5), 
                                              max(lexICA5), 
                                              length = 10),
                            lexICA1 = median(lexICA1),
                            lexICA2 = median(lexICA2),
                            lexICA3 = median(lexICA3),
                            lexICA4 = median(lexICA4),
                            lexICA6 = median(lexICA6),
                            lexICA7 = median(lexICA7),
                            lexICA8 = median(lexICA8),
                            lexICA9 = median(lexICA9),
                            IU.speech.rate.graph.per.sec_c = median(IU.speech.rate.graph.per.sec_c),
                            IU_len_ms_c = median(IU_len_ms_c),
                            BigSurp_c = median(BigSurp_c),
                            Det = "a",
                            Speaker = "AL",
                            Lemma = "air",
                            Animacy.simplified = "inanimate",
                            facPos = "initial"))

# Generate predicted values in log-odds (without random effects)
pred = predict(duration.in.position, 
               new_data, 
               type = "response", 
               se.fit = TRUE, 
               exclude = c("s(Speaker)", "s(Lemma)", "Det", "Animacy.simplified"))

# Add the predicted values to our hypothetical dataframe
pred = cbind(pred, new_data)

# Add CI
pred$lwr_ci = pred$fit - (2 * pred$se.fit)
pred$upr_ci = pred$fit + (2 * pred$se.fit)

# Plot the result
d.init = ggplot(pred, aes(x = lexICA5, y = fit)) +
         geom_ribbon(aes(ymin = lwr_ci, ymax = upr_ci), 
                     fill = "dodgerblue",
                     color = "dodgerblue",
                     alpha = 0.2) +
         geom_line() +
         xlab("Component 5 (syntactic typicality →)") +
         ylab("Duration (ms)") +
         ggtitle("Initial") +
         theme_bw() +
         theme(text = element_text(family="Times New Roman",
                                   size=14,
                                   face="bold"),
               plot.title = element_text(hjust=0.5))

d.init

ggsave(d.init, file = "../Results/comp5_initial_plot.png", units="in", width = 5, height = 3.5, device = "png")
```
  
*4.2.2 Component 5, medial position*
```{r duration_in_pos_medial_plot}
new_data = with(orig_data, 
                expand.grid(lexICA5 = seq(min(lexICA5), 
                                              max(lexICA5), 
                                              length = 10),
                            lexICA1 = median(lexICA1),
                            lexICA2 = median(lexICA2),
                            lexICA3 = median(lexICA3),
                            lexICA4 = median(lexICA4),
                            lexICA6 = median(lexICA6),
                            lexICA7 = median(lexICA7),
                            lexICA8 = median(lexICA8),
                            lexICA9 = median(lexICA9),
                            IU.speech.rate.graph.per.sec_c = median(IU.speech.rate.graph.per.sec_c),
                            IU_len_ms_c = median(IU_len_ms_c),
                            BigSurp_c = median(BigSurp_c),
                            Det = "a",
                            Speaker = "AL",
                            Lemma = "air",
                            Animacy.simplified = "inanimate",
                            facPos = "medial"))

# Generate predicted values in log-odds (without random effects)
pred = predict(duration.in.position, 
               new_data, 
               type = "response", 
               se.fit = TRUE, 
               exclude = c("s(Speaker)", "s(Lemma)", "Det", "Animacy.simplified"))

# Add the predicted values to our hypothetical dataframe
pred = cbind(pred, new_data)

# Add CI
pred$lwr_ci = pred$fit - (2 * pred$se.fit)
pred$upr_ci = pred$fit + (2 * pred$se.fit)

# Plot the result
d.medi = ggplot(pred, aes(x = lexICA5, y = fit)) +
         geom_ribbon(aes(ymin = lwr_ci, ymax = upr_ci), 
                     fill = "dodgerblue",
                     color = "dodgerblue",
                     alpha = 0.2) +
         geom_line() +
         xlab("Component 5 (syntactic typicality →)") +
         ylab("Duration (ms)") +
         ggtitle("Medial") +
         theme_bw() +
         theme(text = element_text(family="Times New Roman",
                                   size=14,
                                   face="bold"),
               plot.title = element_text(hjust=0.5))

d.medi

ggsave(d.medi, file = "../Results/comp5_medial_plot.png", units="in", width = 5, height = 3.5, device = "png")
```
  
*4.2.3 Component 5, final position*
```{r duration_in_pos_final_plot}
new_data = with(orig_data, 
                expand.grid(lexICA5 = seq(min(lexICA5), 
                                              max(lexICA5), 
                                              length = 10),
                            lexICA1 = median(lexICA1),
                            lexICA2 = median(lexICA2),
                            lexICA3 = median(lexICA3),
                            lexICA4 = median(lexICA4),
                            lexICA6 = median(lexICA6),
                            lexICA7 = median(lexICA7),
                            lexICA8 = median(lexICA8),
                            lexICA9 = median(lexICA9),
                            IU.speech.rate.graph.per.sec_c = median(IU.speech.rate.graph.per.sec_c),
                            IU_len_ms_c = median(IU_len_ms_c),
                            BigSurp_c = median(BigSurp_c),
                            Det = "a",
                            Speaker = "AL",
                            Lemma = "air",
                            Animacy.simplified = "inanimate",
                            facPos = "final"))

# Generate predicted values in log-odds (without random effects)
pred.5.final = predict(duration.in.position, 
               new_data, 
               type = "response", 
               se.fit = TRUE, 
               exclude = c("s(Speaker)", "s(Lemma)", "Det", "Animacy.simplified"))

# Add the predicted values to our hypothetical dataframe
pred.5.final = cbind(pred.5.final, new_data)

# Add CI
pred.5.final$lwr_ci = pred.5.final$fit - (2 * pred.5.final$se.fit)
pred.5.final$upr_ci = pred.5.final$fit + (2 * pred.5.final$se.fit)

# Plot the result
d.fina = ggplot(pred.5.final, aes(x = lexICA5, y = fit)) +
         geom_ribbon(aes(ymin = lwr_ci, ymax = upr_ci), 
                     fill = "dodgerblue",
                     color = "dodgerblue",
                     alpha = 0.2) +
         geom_line() +
         xlab("Component 5 (syntactic typicality →)") +
         ylab("Duration (ms)") +
         ggtitle("Final") +
         theme_bw() +
         theme(text = element_text(family="Times New Roman",
                                   size=14,
                                   face="bold"),
               plot.title = element_text(hjust=0.5))


d.fina
```
  
*4.2.4 Component 2, initial position*
```{r component_2_initial_plot}
new_data = with(orig_data, 
                expand.grid(lexICA2 = seq(min(lexICA2), 
                                              max(lexICA2), 
                                              length = 10),
                            lexICA1 = median(lexICA1),
                            lexICA5 = median(lexICA5),
                            lexICA3 = median(lexICA3),
                            lexICA4 = median(lexICA4),
                            lexICA6 = median(lexICA6),
                            lexICA7 = median(lexICA7),
                            lexICA8 = median(lexICA8),
                            lexICA9 = median(lexICA9),
                            IU.speech.rate.graph.per.sec_c = median(IU.speech.rate.graph.per.sec_c),
                            IU_len_ms_c = median(IU_len_ms_c),
                            BigSurp_c = median(BigSurp_c),
                            Det = "a",
                            Speaker = "AL",
                            Lemma = "air",
                            Animacy.simplified = "inanimate",
                            facPos = "initial"))

# Generate predicted values in log-odds (without random effects)
pred = predict(duration.in.position, 
               new_data, 
               type = "response", 
               se.fit = TRUE, 
               exclude = c("s(Speaker)", "s(Lemma)", "Det", "Animacy.simplified"))

# Add the predicted values to our hypothetical dataframe
pred = cbind(pred, new_data)

# Add CI
pred$lwr_ci = pred$fit - (2 * pred$se.fit)
pred$upr_ci = pred$fit + (2 * pred$se.fit)

# Plot the result
d.init.comp2 = ggplot(pred, aes(x = -lexICA2, y = fit)) +
         geom_ribbon(aes(ymin = lwr_ci, ymax = upr_ci), 
                     fill = "dodgerblue",
                     color = "dodgerblue",
                     alpha = 0.2) +
         geom_line() +
         xlab("Component 2 (syntactic diversity + frequency →)") +
         ylab("Duration (ms)") +
         ggtitle("Initial") +
         theme_bw() +
         theme(text = element_text(family="Times New Roman",
                                   size=14,
                                   face="bold"),
               plot.title = element_text(hjust=0.5)) 

d.init.comp2
```
  
*4.2.5 Component 2, medial position*
```{r component_2_medial_plot}
new_data = with(orig_data, 
                expand.grid(lexICA2 = seq(min(lexICA2), 
                                              max(lexICA2), 
                                              length = 10),
                            lexICA1 = median(lexICA1),
                            lexICA5 = median(lexICA5),
                            lexICA3 = median(lexICA3),
                            lexICA4 = median(lexICA4),
                            lexICA6 = median(lexICA6),
                            lexICA7 = median(lexICA7),
                            lexICA8 = median(lexICA8),
                            lexICA9 = median(lexICA9),
                            IU.speech.rate.graph.per.sec_c = median(IU.speech.rate.graph.per.sec_c),
                            IU_len_ms_c = median(IU_len_ms_c),
                            BigSurp_c = median(BigSurp_c),
                            Det = "a",
                            Speaker = "AL",
                            Lemma = "air",
                            Animacy.simplified = "inanimate",
                            facPos = "medial"))

# Generate predicted values in log-odds (without random effects)
pred = predict(duration.in.position, 
               new_data, 
               type = "response", 
               se.fit = TRUE, 
               exclude = c("s(Speaker)", "s(Lemma)", "Det", "Animacy.simplified"))

# Add the predicted values to our hypothetical dataframe
pred = cbind(pred, new_data)

# Add CI
pred$lwr_ci = pred$fit - (2 * pred$se.fit)
pred$upr_ci = pred$fit + (2 * pred$se.fit)

# Plot the result
d.medi.comp2 = ggplot(pred, aes(x = -lexICA2, y = fit)) +
         geom_ribbon(aes(ymin = lwr_ci, ymax = upr_ci), 
                     fill = "dodgerblue",
                     color = "dodgerblue",
                     alpha = 0.2) +
         geom_line() +
         xlab("Component 2 (syntactic diversity + frequency →)") +
         ylab("Duration (ms)") +
         ggtitle("Medial") +
         theme_bw() +
         theme(text = element_text(family="Times New Roman",
                                   size=14,
                                   face="bold"),
               plot.title = element_text(hjust=0.5)) 

d.medi.comp2

ggsave(d.medi.comp2, file = "../Results/comp2_medial_plot.png", units="in", width = 5, height = 3.5, device = "png")
```
  
*4.2.6 Component 2, final position*
```{r component_2_final_plot}
new_data = with(orig_data, 
                expand.grid(lexICA2 = seq(min(lexICA2), 
                                              max(lexICA2), 
                                              length = 10),
                            lexICA1 = median(lexICA1),
                            lexICA5 = median(lexICA5),
                            lexICA3 = median(lexICA3),
                            lexICA4 = median(lexICA4),
                            lexICA6 = median(lexICA6),
                            lexICA7 = median(lexICA7),
                            lexICA8 = median(lexICA8),
                            lexICA9 = median(lexICA9),
                            IU.speech.rate.graph.per.sec_c = median(IU.speech.rate.graph.per.sec_c),
                            IU_len_ms_c = median(IU_len_ms_c),
                            BigSurp_c = median(BigSurp_c),
                            Det = "a",
                            Speaker = "AL",
                            Lemma = "air",
                            Animacy.simplified = "inanimate",
                            facPos = "final"))

# Generate predicted values in log-odds (without random effects)
pred.2.final = predict(duration.in.position, 
                       new_data, 
                       type = "response", 
                       se.fit = TRUE, 
                       exclude = c("s(Speaker)", "s(Lemma)", "Det", "Animacy.simplified"))

# Add the predicted values to our hypothetical dataframe
pred.2.final = cbind(pred.2.final, new_data)

# Add CI
pred.2.final$lwr_ci = pred.2.final$fit - (2 * pred.2.final$se.fit)
pred.2.final$upr_ci = pred.2.final$fit + (2 * pred.2.final$se.fit)

# Plot the result
d.fina.comp2 = ggplot(pred.2.final, aes(x = -lexICA2, y = fit)) +
         geom_ribbon(aes(ymin = lwr_ci, ymax = upr_ci), 
                     fill = "dodgerblue",
                     color = "dodgerblue",
                     alpha = 0.2) +
         geom_line() +
         xlab("Component 2 (syntactic diversity + frequency →)") +
         ylab("Duration (ms)") +
         ggtitle("Final") +
         theme_bw() +
         theme(text = element_text(family="Times New Roman",
                                   size=14,
                                   face="bold"),
               plot.title = element_text(hjust=0.5)) 

d.fina.comp2
```
  
**NB**: To aid interpretation, we reverse the sign of the Component 2 values. The arrow in the x-axis label indicates how the variables increase relative to the component scores. 
  
**Randomly clipping lengths to simulate removal of final segment**  
An anonymous reviewer raised the possibility that our pre-trained forced alignment model could suffer from biases in the identification of segment boundaries, particularly in word-final position. The threat is presumably more serious in phrase-final position as multiple sources push for lengthening.
  
To address this point, we repeatedly clip the durations of individual words by a randomly specified percentage of overall duration in ms. For example, let's say we have a token of the word *beads* [bidz] with true duration 150ms might be randomly assigned a clipping value of .2 (1/5 of overall duration to be deleted). We multiply the true duration by the clipping value, then subtract the result from the true duration:  

$$
\begin{align*}
.2 * 150ms & = 30ms \\  
150ms - 130ms & = 120ms \\
\end{align*}
$$
  
The new time (120 ms) is meant to represent the word's duration after the final segment has been clipped (i.e., the duration of [bid]), under the assumption that [z] took up 30ms of the overall duration. (since cutting the same amount of time from any position in the word results in the same net decrease in duration, we can assume that our cuts apply to the right edge).  
  
There is no reason, of course, to believe that the randomly assigned proportion is the true proportion. But if we iterate this process again and again, we can explore to what extent random-end-character truncation impacts the shape of the relationship between our critical variables and the position/duration-within-position values. 
```{r random_truncation}
# Create vector of length n of randomly 
# selected proportions between lower-bound 
# "min" and upper-bound "max" arguments
rand.prop.vec = function(n, min, max){
    # Establish the set of possible values (with 
    # precision of .01)
    possible.values = seq(from=min, to=max, by=.01)

    # Create the random vector of clipping proportions
    # by sampling n tokens with replacement
    rand.values = sample(possible.values, n, replace=T)
    
    return(rand.values)
}

# Function to perform the modeling
modeling = function(mod.dat, formula, type){
    if(type=="binomial"){
        m = bam(formula, data=mod.dat, family="binomial")
    }
    else if(type=="continuous"){
        m = bam(formula, data=mod.dat)
    }
    return(m)
}

# Iterate the process of randomly clipping
rand.clipper = function(df, target.var, min.cut, max.cut, k, type, formula, syll=F){
    # Initialize list to collect models for each 
    # iteration (pre-initializing length
    # to the number of iterations)
    mod.list = vector(mode="list", length=k)
    
    # Iterator token
    i = 1
    
    for(iter.num in 1:k){
        # Get the random vector
        rand.vec = rand.prop.vec(nrow(df), 
                                 min.cut,
                                 max.cut)
        
        # Find the amount to clip per token
        amount.to.clip = rand.vec * df$target.duration.ms
        
        # Clip
        df$rand.target.duration.ms = df$target.duration.ms - amount.to.clip
        
        # Length and speech rate need to 
        # be recalculated based on the clip
        
        # IU length (ms)
        df$rand.IU_len_ms = df$IU_len_ms - amount.to.clip
        
        # (and scaled IU length)
        df$rand.IU_len_ms_c = scale(df$rand.IU_len_ms)
        
        # Speech rate
        df$rand.IU.speech.rate.graph.per.sec = df$nletters/(df$rand.IU_len_ms/1000)
        
        df$rand.IU.speech.rate.syll.per.sec = 1/(df$rand.IU_len_ms/1000)
        
        # (and scaled speech rate)
        df$rand.IU.speech.rate.graph.per.sec_c = scale(df$rand.IU.speech.rate.graph.per.sec)
        
        df$rand.IU.speech.rate.syll.per.sec_c = scale(df$rand.IU.speech.rate.syll.per.sec)
        
        # Model
        m = modeling(df, formula, type)
        
        # Extract results
        m.results = predicted.values(m, df, target.var, syll)

        # Add the iteration number
        m.results$iteration = as.factor(rep(i, nrow(m.results)))
        
        # Save the model and predicted values
        mod.list[[i]] = list(m, m.results)

        i = i + 1
        
    }
    
    return(mod.list)
    
}

# Generate predicted values
predicted.values = function(model, model.data, target.var, syll=F){
    if(target.var == "lexICA2"){
        tv = model.data$lexICA2
        ica2 = seq(min(tv),
                   max(tv),
                   length = 10)
        ica4 = median(model.data$lexICA4)
        ica5 = median(model.data$lexICA5)
    }
    else if(target.var == "lexICA4"){
        tv = model.data$lexICA4
        ica4 = seq(min(tv),
                   max(tv),
                   length = 10)
        ica2 = median(model.data$lexICA2)
        ica5 = median(model.data$lexICA5)
    }
    else if(target.var == "lexICA5"){
        tv = model.data$lexICA5
        ica5 = seq(min(tv),
                   max(tv),
                   length = 10)
        ica2 = median(model.data$lexICA2)
        ica4 = median(model.data$lexICA4)
    }
  
    new_data = with(model.data, 
                    expand.grid(lexICA1 = median(lexICA1),
                                lexICA2 = ica2,
                                lexICA3 = median(lexICA3),
                                lexICA4 = ica4,
                                lexICA5 = ica5,
                                lexICA6 = median(lexICA6),
                                lexICA7 = median(lexICA7),
                                lexICA8 = median(lexICA8),
                                lexICA9 = median(lexICA9),
                                rand.IU.speech.rate.graph.per.sec_c = median(rand.IU.speech.rate.graph.per.sec_c),
                                rand.IU_len_ms_c = median(rand.IU_len_ms_c),
                                BigSurp_c = median(BigSurp_c),
                                Det = "a",
                                Speaker = "AL",
                                Lemma = "air",
                                Animacy.simplified = "inanimate",
                                facPos = "final"))
    if(syll){
    # If using syllable-based speech rate estimates. 
    # change graph/s to syllable/s.
        new_data = new_data %>% 
                   rename(rand.IU.speech.rate.syll.per.sec_c = rand.IU.speech.rate.graph.per.sec_c) %>%
                   mutate(rand.IU.speech.rate.syll.per.sec_c = median(model.data$rand.IU.speech.rate.syll.per.sec_c))
    }
    # Generate predicted values in log-odds       (without random effects)
    pred = predict(model, 
                   new_data, 
                   type = "response", 
                   se.fit = TRUE, 
                   exclude = c("s(Speaker)", 
                               "s(Lemma)", 
                               "Det", 
                               "Animacy.simplified"))

    # Add the predicted values to our hypothetical dataframe
    pred = cbind(pred, new_data)

    # Add CI
    pred$lwr_ci = pred$fit - (2 * pred$se.fit)
    pred$upr_ci = pred$fit + (2 * pred$se.fit)
    
    # Add label for which variable we are looking at
    pred$target.variable = rep(target.var, nrow(pred))
    
    return(pred)
}

# Plotting function
plot.iters = function(pred, target.var){
    # get shorthand name of target variable
    tvar = gsub("lex", "", target.var)
    
    # Plot the result
    p = ggplot(pred, aes(x = pred %>% 
                             dplyr::select(target.var) %>% 
                             pull, 
                         y = fit, 
                         fill=iteration, 
                         color=iteration)) +
        geom_ribbon(aes(ymin = lwr_ci, 
                        ymax = upr_ci), 
                    alpha = 0.2) +
        geom_line() +
        xlab(tvar) +
        ylab("Duration (ms)") +
        ggtitle("Randomly snipped durations") +
        theme_bw() +
        theme(text = element_text(family="Times New Roman",
                                   size=14,
                                   face="bold"),
               plot.title = element_text(hjust=0.5))

    return(p)
}

stars = function(x){
    if(x>0.01 & x<=0.05){
        star = "*"
    }
    else if(x>0.001 & x<=0.01){
        star = "**"
    }
    else if(x<=0.001){
        star = "***"
    }
    else{
        star = ""
    }
    return(star)
}

p.grabber = function(model.obj, var){
    if(var=="lexICA2"){
        ind = 4
    }
    else if(var=="lexICA4"){
        ind = 10
    }
    else if(var=="lexICA5"){
        ind = 13
    }
    all.ps = summary(model.obj)$s.pv
    target.p = all.ps[ind]
    star = stars(target.p)
    return(star)
}

post.processing = function(mod.list, var, orig.dat){
    # get data for plotting
    clip.preds = do.call(rbind, lapply(mod.list, function(x){ as.data.frame(x[[2]])}))

    # extract p-values
    p.vals = unlist(lapply(mod.list, function(x){p.grabber(x[[1]], var)}))
    
    # fix row names
    rownames(clip.preds) = seq(1:nrow(clip.preds))
    
    # fix column names
    colnames(clip.preds) = gsub("^rand.", 
                                "",
                                colnames(clip.preds),
                                perl=T)

    # add necessary variables to full model results   
    orig.result = orig.dat %>%
                  mutate(target.variable = rep(var, nrow(.)), 
                         iteration = rep("0 (full model)", nrow(.)))

    # Construct data for plotting
    plot.dat = bind_rows(orig.result,
                         clip.preds) %>%
               mutate(iteration = as.factor(iteration))
    
    ## relevel factor for prettier legend
    plot.dat$iteration = factor(plot.dat$iteration, 
                                levels = levels(plot.dat$iteration)[c(1, 2, 4:11, 3)])

    
    return(list(plot.dat, p.vals, clip.preds))
}
```
  
Formulae to use in the resampling
```{r formulae}
# Duration in position
dur.in.pos.f.rand = formula(rand.target.duration.ms ~ Det + Animacy.simplified + s(lexICA1, by=facPos) + s(lexICA2, by=facPos) + s(lexICA3, by=facPos) + s(lexICA4, by=facPos) + s(lexICA5, by=facPos) + s(lexICA6, by=facPos) + s(lexICA7, by=facPos) + s(lexICA8, by=facPos) + s(lexICA9, by=facPos) + s(rand.IU.speech.rate.graph.per.sec_c, by=facPos) + s(rand.IU_len_ms_c, by=facPos) + s(BigSurp_c, by=facPos) + s(Lemma, bs="re") + s(Speaker, bs="re"))

# Duration in position syll/s
dur.in.pos.wSyll.f.rand = formula(rand.target.duration.ms ~ Det + Animacy.simplified + s(lexICA1, by=facPos) + s(lexICA2, by=facPos) + s(lexICA3, by=facPos) + s(lexICA4, by=facPos) + s(lexICA5, by=facPos) + s(lexICA6, by=facPos) + s(lexICA7, by=facPos) + s(lexICA8, by=facPos) + s(lexICA9, by=facPos) + s(rand.IU.speech.rate.syll.per.sec_c, by=facPos) + s(rand.IU_len_ms_c, by=facPos) + s(BigSurp_c, by=facPos) + s(Lemma, bs="re") + s(Speaker, bs="re"))
```
  
Proceed to clip the durations:
```{r clip.durations}
set.seed(1986)

# ICA2 models
ica2.mods.25 = rand.clipper(mod_dat[mod_dat$target.duration.ms<750,], target.var = "lexICA2", min.cut = 0.05, max.cut=0.2, k=10, type="continuous", formula = dur.in.pos.f.rand)

# ICA4 models
ica4.mods.25 = rand.clipper(mod_dat[mod_dat$target.duration.ms<750,], target.var = "lexICA4", min.cut = 0.05, max.cut=0.2, k=10, type="continuous", formula = dur.in.pos.f.rand)

# ICA5 models
ica5.mods.25 = rand.clipper(mod_dat[mod_dat$target.duration.ms<750,], target.var = "lexICA5", min.cut = 0.05, max.cut=0.2, k=10, type="continuous", formula = dur.in.pos.f.rand)
```
  
Now investigate the behavior of each variable via some plots
```{r plotting.clipped}

# ICA2
## 0.2-0.5 clipping
ica2.25.pp = post.processing(ica2.mods.25, "lexICA2", pred.2.final)

# NB -- cleaning out some outliers (n=3) to smooth the effect
ica2.25.p = plot.iters(ica2.25.pp[[1]] %>% 
                    filter(fit<400 & 
                           fit>130),
                    target.var = "lexICA2") +
         scale_fill_discrete(labels = paste0(levels(ica2.25.pp[[1]]$iteration), c("", ica2.25.pp[[2]]))) +
         scale_color_discrete(labels = paste0(levels(ica2.25.pp[[1]]$iteration), c("", ica2.25.pp[[2]])))


ica2.25.p

# ICA5
## 0.2-0.5 clipping
ica5.pp = post.processing(ica5.mods.25, "lexICA5", pred.5.final)

# NB -- cleaning out some outliers (n=3) to smooth the effect
ica5.p = plot.iters(ica5.pp[[1]] %>% 
                    filter(fit<400 & 
                           fit>130),
                    target.var = "lexICA5") +
         scale_fill_discrete(labels = paste0(levels(ica5.pp[[1]]$iteration), c("", ica5.pp[[2]]))) +
         scale_color_discrete(labels = paste0(levels(ica5.pp[[1]]$iteration), c("", ica5.pp[[2]])))


ica5.p

```

```{r clip.durations.allSame}
#########
# 2% cuts
#########
# ICA2 models
ica2.mods.2p = rand.clipper(mod_dat[mod_dat$target.duration.ms<750,], target.var = "lexICA2", min.cut = 0.02, max.cut=0.02, k=10, type="continuous", formula = dur.in.pos.f.rand)

ica2.2p.pp = post.processing(ica2.mods.2p, "lexICA2", pred.2.final)

ica2.2p.p = plot.iters(ica2.2p.pp[[1]] %>% 
                    filter(fit<400 & 
                           fit>130 &
                            iteration %in% c("0 (full model)", "1")),
                    target.var = "lexICA2") +
         scale_fill_discrete(labels = paste0(levels(ica2.2p.pp[[1]]$iteration), c("", ica2.2p.pp[[2]]))) +
         scale_color_discrete(labels = paste0(levels(ica2.2p.pp[[1]]$iteration), c("", ica2.2p.pp[[2]]))) + 
         ggtitle("2% cut to duration")


ica2.2p.p

# ICA5 models
ica5.mods.2p = rand.clipper(mod_dat[mod_dat$target.duration.ms<750,], target.var = "lexICA5", min.cut = 0.02, max.cut=0.02, k=10, type="continuous", formula = dur.in.pos.f.rand)

ica5.2p.pp = post.processing(ica5.mods.2p, "lexICA5", pred.5.final)

ica5.2p.p = plot.iters(ica5.2p.pp[[1]] %>% 
                    filter(fit<400 & 
                           fit>130 &
                            iteration %in% c("0 (full model)", "1")),
                    target.var = "lexICA5") +
         scale_fill_discrete(labels = paste0(levels(ica5.2p.pp[[1]]$iteration), c("", ica5.2p.pp[[2]]))) +
         scale_color_discrete(labels = paste0(levels(ica5.2p.pp[[1]]$iteration), c("", ica5.2p.pp[[2]]))) + 
         ggtitle("2% cut to duration")


ica5.2p.p

#########
# 5% cuts
#########
# ICA2 models
ica2.mods.5p = rand.clipper(mod_dat[mod_dat$target.duration.ms<750,], target.var = "lexICA2", min.cut = 0.05, max.cut=0.05, k=10, type="continuous", formula = dur.in.pos.f.rand)

ica2.5p.pp = post.processing(ica2.mods.5p, "lexICA2", pred.2.final)

ica2.5p.p = plot.iters(ica2.5p.pp[[1]] %>% 
                    filter(fit<400 & 
                           fit>130 &
                            iteration %in% c("0 (full model)", "1")),
                    target.var = "lexICA2") +
         scale_fill_discrete(labels = paste0(levels(ica2.5p.pp[[1]]$iteration), c("", ica2.5p.pp[[2]]))) +
         scale_color_discrete(labels = paste0(levels(ica2.5p.pp[[1]]$iteration), c("", ica2.5p.pp[[2]]))) + 
         ggtitle("5% cut to duration")


ica2.5p.p

# ICA5 models
ica5.mods.5p = rand.clipper(mod_dat[mod_dat$target.duration.ms<750,], target.var = "lexICA5", min.cut = 0.05, max.cut=0.05, k=10, type="continuous", formula = dur.in.pos.f.rand)

ica5.5p.pp = post.processing(ica5.mods.5p, "lexICA5", pred.5.final)

ica5.5p.p = plot.iters(ica5.5p.pp[[1]] %>% 
                    filter(fit<400 & 
                           fit>130 &
                            iteration %in% c("0 (full model)", "1")),
                    target.var = "lexICA5") +
         scale_fill_discrete(labels = paste0(levels(ica5.5p.pp[[1]]$iteration), c("", ica5.5p.pp[[2]]))) +
         scale_color_discrete(labels = paste0(levels(ica5.5p.pp[[1]]$iteration), c("", ica5.5p.pp[[2]]))) + 
         ggtitle("5% cut to duration")


ica5.5p.p

##########
# 20% cuts
##########
# ICA2 models
ica2.mods.20p = rand.clipper(mod_dat[mod_dat$target.duration.ms<750,], target.var = "lexICA2", min.cut = 0.2, max.cut=0.2, k=10, type="continuous", formula = dur.in.pos.f.rand)

ica2.20p.pp = post.processing(ica2.mods.20p, "lexICA2", pred.2.final)

ica2.20p.p = plot.iters(ica2.20p.pp[[1]] %>% 
                    filter(fit<400 & 
                           fit>130 &
                            iteration %in% c("0 (full model)", "1")),
                    target.var = "lexICA2") +
         scale_fill_discrete(labels = paste0(levels(ica2.20p.pp[[1]]$iteration), c("", ica2.20p.pp[[2]]))) +
         scale_color_discrete(labels = paste0(levels(ica2.20p.pp[[1]]$iteration), c("", ica2.20p.pp[[2]]))) + 
         ggtitle("20% cut to duration")


ica2.20p.p

# ICA5 models
ica5.mods.20p = rand.clipper(mod_dat[mod_dat$target.duration.ms<750,], target.var = "lexICA5", min.cut = 0.2, max.cut=0.2, k=10, type="continuous", formula = dur.in.pos.f.rand)

ica5.20p.pp = post.processing(ica5.mods.20p, "lexICA5", pred.5.final)

ica5.20p.p = plot.iters(ica5.20p.pp[[1]] %>% 
                    filter(fit<400 & 
                           fit>130 &
                            iteration %in% c("0 (full model)", "1")),
                    target.var = "lexICA5") +
         scale_fill_discrete(labels = paste0(levels(ica5.20p.pp[[1]]$iteration), c("", ica5.20p.pp[[2]]))) +
         scale_color_discrete(labels = paste0(levels(ica5.20p.pp[[1]]$iteration), c("", ica5.20p.pp[[2]]))) + 
         ggtitle("20% cut to duration")


ica5.20p.p

##########
# 50% cuts
##########
# ICA2 models
ica2.mods.50p = rand.clipper(mod_dat[mod_dat$target.duration.ms<750,], target.var = "lexICA2", min.cut = 0.5, max.cut=0.5, k=10, type="continuous", formula = dur.in.pos.f.rand)

ica2.50p.pp = post.processing(ica2.mods.50p, "lexICA2", pred.2.final)

ica2.50p.p = plot.iters(ica2.50p.pp[[1]] %>% 
                    filter(fit<400 & 
                           fit>130 &
                            iteration %in% c("0 (full model)", "1")),
                    target.var = "lexICA2") +
         scale_fill_discrete(labels = paste0(levels(ica2.50p.pp[[1]]$iteration), c("", ica2.50p.pp[[2]]))) +
         scale_color_discrete(labels = paste0(levels(ica2.50p.pp[[1]]$iteration), c("", ica2.50p.pp[[2]]))) + 
         ggtitle("50% cut to duration")


ica2.50p.p

# ICA5 models
ica5.mods.50p = rand.clipper(mod_dat[mod_dat$target.duration.ms<750,], target.var = "lexICA5", min.cut = 0.5, max.cut=0.5, k=10, type="continuous", formula = dur.in.pos.f.rand)

ica5.50p.pp = post.processing(ica5.mods.50p, "lexICA5", pred.5.final)

ica5.50p.p = plot.iters(ica5.50p.pp[[1]] %>% 
                    filter(fit<400 & 
                           fit>130 &
                            iteration %in% c("0 (full model)", "1")),
                    target.var = "lexICA5") +
         scale_fill_discrete(labels = paste0(levels(ica5.50p.pp[[1]]$iteration), c("", ica5.50p.pp[[2]]))) +
         scale_color_discrete(labels = paste0(levels(ica5.50p.pp[[1]]$iteration), c("", ica5.50p.pp[[2]]))) + 
         ggtitle("50% cut to duration")


ica5.50p.p
```